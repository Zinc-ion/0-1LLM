import argparse
import random
import time
import numpy as np
import torch
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM
from model import SpongeBob
from Config import LLMConfig


def init_model(args):
     tokenizer = AutoTokenizer.from_pretrained('./spongebob_tokenizer')
     if args.model_mode == 1:
          ckp = f'./{args.save_dir}/SFT.pth'
     if args.model_mode == 0:
          ckp = f'./{args.save_dir}/pretrain.pth'
     if args.model_mode ==2:
          ckp = f'./{args.save_dir}/distill.pth'
     if args.model_mode ==3:
          ckp = f'./{args.save_dir}/SFT_final_eval_mini512.pth'
     if args.model_mode ==4:
          ckp = f'./{args.save_dir}/pretrain_step15999_eval_ep2_bs84_acu2.pth'
     if args.model_mode ==5:
          ckp = f'./{args.save_dir}/distill_long.pth'

     print(f'åŠ è½½æ¨¡å‹: {ckp}')

     model = SpongeBob(LLMConfig(
          max_seq_len=args.max_seq_len,
     ))

     state_dict = torch.load(ckp, map_location=args.device)
     model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=True)
     # æ’é™¤maskï¼Œå¦‚æœkä¸­æ²¡æœ‰maskï¼Œåˆ™åŠ è½½ï¼Œå¦åˆ™ä¸åŠ è½½

     print(f'æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M(illion)')
     return model.eval().to(args.device), tokenizer




def main():
     parser=argparse.ArgumentParser()
     parser.add_argument('--save_dir', default='results', type=str)
     parser.add_argument('--temperature', default=0.7, type=float)
     parser.add_argument('--top_p', default=0.8, type=float)
     parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
     parser.add_argument('--max_seq_len', default=8192, type=int)
     parser.add_argument('--history_cnt', default=0, type=int)
     parser.add_argument('--stream', default=True, type=bool)
     parser.add_argument('--model_mode', default=1, type=int,
                        help="0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹")
     
     args = parser.parse_args()
     model, tokenizer = init_model(args)
     messages=[]
     while True:
          # è·å–ç”¨æˆ·è¾“å…¥
          prompt = input('ğŸ‘¶: ')  # æ‰‹åŠ¨è¾“å…¥å¯¹è¯å†…å®¹ ğŸ‘¶ ç¬¦å·æç¤ºç”¨æˆ·è¾“å…¥ input()å‡½æ•°æ¥æ”¶ç”¨æˆ·è¾“å…¥
          # ä¿ç•™æœ€è¿‘Næ¡å¯¹è¯å†å²ï¼ˆN=0æ—¶æ¸…ç©ºå†å²ï¼‰
          messages = messages[-args.history_cnt:] if args.history_cnt else []   # -args.history_cnt: ä»åå¾€å‰å–Næ¡å†å²è®°å½•
          # [-args.history_cnt:]è¡¨ç¤ºä»åå¾€å‰å–Næ¡å†å²è®°å½• ä»å‰å¾€åå–çš„è¯æ˜¯[:args.history_cnt]
          # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
          messages.append({"role": "user", "content": prompt})

          # å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼ˆä¸¤ç§æ¨¡å¼ï¼‰
          # print('messages:', messages)
          new_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True    # SFTæ¨¡å¼                     # é¢„è®­ç»ƒæ¨¡å¼
          )[-args.max_seq_len + 1:] if args.model_mode != 0 else (tokenizer.bos_token + prompt)
          # print('new_prompt:', new_prompt)

          # æ¨¡å‹æ¨ç†
          with torch.no_grad():   # å…³é—­æ¢¯åº¦è®¡ç®—
               x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=args.device).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
               outputs = model.generate(
                    x,
                    eos_token_id=tokenizer.eos_token_id,   # ç»ˆæ­¢ç¬¦å·ID
                    max_new_tokens=args.max_seq_len,    # æœ€å¤§ç”Ÿæˆé•¿åº¦
                    temperature=args.temperature,    # æ¸©åº¦å‚æ•°
                    top_p=args.top_p,   # top-pé‡‡æ ·å‚æ•° æ ¸å¿ƒé‡‡æ ·ç­–ç•¥
                    stream=True,   # æµå¼ç”Ÿæˆ
                    pad_token_id=tokenizer.pad_token_id,    # å¡«å……ç¬¦å·ID
                    rp=1.3    # é‡å¤æƒ©ç½šå‚æ•°
               )

               # æµå¼è¾“å‡ºå¤„ç†
               print('ğŸ¤–ï¸: ', end='')    # ğŸ¤–ï¸ è¡¨ç¤ºæœºå™¨äººå›å¤å¼€å§‹
               # end=''è¡¨ç¤ºä¸æ¢è¡Œ
               try:
                    history_idx = 0     # å·²è¾“å‡ºæ–‡æœ¬é•¿åº¦æ ‡è®°
                    for y in outputs:
                         answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=False)   # è§£ç å½“å‰è¾“å‡º
                         if (answer and answer[-1] == 'ï¿½') or not answer:  # è¿‡æ»¤æ— æ•ˆå­—ç¬¦
                              continue
                         print(answer[history_idx:], end='', flush=True)   # é€å­—è¾“å‡º åœ¨Pythonä¸­ï¼Œ flush=True å‚æ•°ç”¨äºå¼ºåˆ¶ç«‹å³æ¸…ç©ºè¾“å‡ºç¼“å†²åŒºã€‚
                         history_idx = len(answer)     # æ›´æ–°å·²è¾“å‡ºé•¿åº¦
               except StopIteration:
                    print("No answer")  # å¼‚å¸¸å¤„ç†
               print('\n')

          messages.append({"role": "assistant", "content": answer})   # æ›´æ–°å¯¹è¯å†å²


if __name__ == "__main__":
    main()